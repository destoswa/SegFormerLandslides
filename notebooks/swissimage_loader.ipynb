{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e330343",
   "metadata": {},
   "source": [
    "# SWISSIMAGE loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ea7bc",
   "metadata": {},
   "source": [
    "1) [Production of extents](#production-of-extents) \\\n",
    "    1.1) [Loading and data visualization](#loading-and-data-visualization) \\\n",
    "    1.2) [Add flight_year to points](#add-flight_year-to-points) \\\n",
    "    1.2) [Add flight_year to merged polygons](#add-flight_year-to-merged-polygons) \n",
    "2) [Processing of extents](#processing-of-extents) \\\n",
    "    2.1) [Binarization of masks](#binarization-of-masks) \\\n",
    "    2.2) [Splitting images into 512x512 subset](#splitting-images-into-512x512-subset) \\\n",
    "    2.3) [Remove tiles that don't contain landslides](#remove-tiles-that-dont-contain-landslides) \\\n",
    "    2.4) [Sort into subfolders](#sort-into-subfolders)\n",
    "3) [Find mask with different years](#find-mask-with-different-years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67deb97c",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bee5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d09d5",
   "metadata": {},
   "source": [
    "## Production of extents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e5b9d",
   "metadata": {},
   "source": [
    "### Loading and data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a08939",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_polygons = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\BD glissements spontanés\\BE_SL_all_aggreg_PQ.shp\"\n",
    "src_points_intersect = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\points_landslide_bern_intersect_polygons.gpkg\"\n",
    "src_km_grid = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\swissIMAGE_HIST.gpkg\"\n",
    "src_landslides_merged = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\landslides_anriss_auslauf_merged.gpkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2ba925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of years:  24\n",
      "[1962 1974 1980 1985 1992 1993 1998 1999 2000 2004 2005 2006 2007 2008\n",
      " 2009 2010 2012 2014 2015 2016 2017 2018 2021 2022]\n",
      "Index(['RS_ID', 'year', 'outline', 'source', 'ID_alt', 'Anrissmäc', 'Volumen',\n",
      "       'ID', 'Jahr', 'Datum', 'SHAPE_Leng', 'SHAPE_Area', 'geometry',\n",
      "       'flight_year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "points_intersect = gpd.read_file(src_points_intersect)\n",
    "print(\"Number of years: \", len(points_intersect.year.unique()))\n",
    "print(np.sort(points_intersect.year.unique()))\n",
    "points_intersect['flight_year'] = np.zeros(len(points_intersect))\n",
    "print(points_intersect.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce46d146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         flight_year resolution_of_origin  \\\n",
      "0  1926,1943,1946,1954,1969,1975,1976,1981,1987,1...        50,100,,25,10   \n",
      "1  1926,1943,1946,1954,1955,1956,1969,1975,1976,1...        50,100,,25,10   \n",
      "2  1926,1943,1946,1955,1956,1969,1975,1976,1981,1...        50,100,,25,10   \n",
      "3  1926,1931,1946,1951,1962,1965,1969,1971,1975,1...        50,100,,25,10   \n",
      "4  1926,1943,1946,1955,1956,1969,1975,1976,1981,1...        50,100,,25,10   \n",
      "\n",
      "      id     left      top    right   bottom  \\\n",
      "0  45727  2670000  1206000  2671000  1205000   \n",
      "1  46200  2672000  1201000  2673000  1200000   \n",
      "2  46201  2672000  1200000  2673000  1199000   \n",
      "3  45024  2667000  1207000  2668000  1206000   \n",
      "4  46202  2672000  1199000  2673000  1198000   \n",
      "\n",
      "                                            geometry  \n",
      "0  MULTIPOLYGON (((2670601.819 1206000, 2671000 1...  \n",
      "1  MULTIPOLYGON (((2672000 1201000, 2673000 12010...  \n",
      "2  MULTIPOLYGON (((2672000 1199083.191, 2672000 1...  \n",
      "3  MULTIPOLYGON (((2667000 1206947.07, 2667000 12...  \n",
      "4  MULTIPOLYGON (((2673000 1199000, 2673000 11988...  \n",
      "0        1926,1943,1946,1954,1969,1975,1976,1981,1987,1...\n",
      "1        1926,1943,1946,1954,1955,1956,1969,1975,1976,1...\n",
      "2        1926,1943,1946,1955,1956,1969,1975,1976,1981,1...\n",
      "3        1926,1931,1946,1951,1962,1965,1969,1971,1975,1...\n",
      "4        1926,1943,1946,1955,1956,1969,1975,1976,1981,1...\n",
      "                               ...                        \n",
      "58504                                                 2008\n",
      "58505                                                 2008\n",
      "58506                                                 2008\n",
      "58507                                                 2008\n",
      "58508                                                 2008\n",
      "Name: flight_year, Length: 58509, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load grid\n",
    "grid = gpd.read_file(src_km_grid)\n",
    "print(grid.head())\n",
    "print(grid.flight_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e6a6c",
   "metadata": {},
   "source": [
    "### Add flight_year to points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd6e592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 556/556 [00:03<00:00, 176.64it/s]\n"
     ]
    }
   ],
   "source": [
    "lst_points_too_recent = []\n",
    "lst_flight_years = np.zeros(len(points_intersect), dtype=int)\n",
    "for point in tqdm(points_intersect.itertuples(), total=len(points_intersect)):\n",
    "    intersection_mask = grid.geometry.contains(point.geometry)#.intersection(grid.geometry)\n",
    "    intersection = grid[intersection_mask]\n",
    "\n",
    "    lst_years = []\n",
    "    if np.sum(intersection_mask) != 1:\n",
    "        print(\"Problem (no matching polygon) with : \", point)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        flight_year = np.min([int(x) for x in intersection.flight_year.values[0].split(',') if int(x) >= point.Jahr])\n",
    "    except:\n",
    "        lst_points_too_recent.append(point)\n",
    "        continue\n",
    "\n",
    "    lst_flight_years[point.Index] = flight_year\n",
    "\n",
    "points_intersect['flight_year'] = lst_flight_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e721041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points with year too high for the grid:  15\n",
      "Different flight years:  {np.int64(0), np.int64(1933), np.int64(1934), np.int64(1936), np.int64(1939), np.int64(1940), np.int64(1944), np.int64(1945), np.int64(1946), np.int64(1999), np.int64(2000), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2012), np.int64(2015), np.int64(2016), np.int64(2021)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of points with year too high for the grid: \", np.sum(lst_flight_years == 0))\n",
    "print(\"Different flight years: \", set(points_intersect.flight_year.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb46fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_intersect.to_file(src_points_intersect.split('.gpkg')[0] + '_w_flight_year.gpkg', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f6bb2",
   "metadata": {},
   "source": [
    "### Add flight_year to merged polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cf7dcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/498 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 289/498 [00:01<00:01, 155.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021, 2007, 2007]\n",
      "2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:03<00:00, 150.96it/s]\n"
     ]
    }
   ],
   "source": [
    "merged_polygons = gpd.read_file(src_landslides_merged)\n",
    "# print(merged_polygons.index)points_intersect = gpd.read_file(src_points_intersect.split('.gpkg')[0] + '_w_flight_year.gpkg')\n",
    "points_intersect = points_intersect[points_intersect.flight_year >= 1998]\n",
    "flight_year = np.zeros(len(merged_polygons), dtype=int)\n",
    "list_polygons_too_old = []\n",
    "for polygon in tqdm(merged_polygons.itertuples(), total=len(merged_polygons)):\n",
    "    points = [p for p in points_intersect.itertuples() if polygon.geometry.contains(p.geometry)]\n",
    "    flight_years = [x.flight_year for x in points]\n",
    "    if len(set(flight_years)) == 1:\n",
    "        flight_year[polygon.Index] = flight_years[0]\n",
    "    elif len(set(flight_years)) > 1:\n",
    "        flight_year[polygon.Index] = np.bincount(flight_years).argmax()\n",
    "        print([x.flight_year for x in points])\n",
    "        print(flight_year[polygon.Index])\n",
    "    else:\n",
    "        list_polygons_too_old.append(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1742c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of polygons too old:  140\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of polygons too old: \", len(list_polygons_too_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f869e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_polygons['flight_year'] = flight_year\n",
    "merged_polygons.to_file(src_landslides_merged.split('.gpkg')[0] + \"_w_flight_year.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d164e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different years:  [0, 1999, 2000, 2004, 2005, 2006, 2007, 2012, 2015, 2016, 2021]\n",
      "498\n"
     ]
    }
   ],
   "source": [
    "print(\"Different years: \", sorted(set(merged_polygons.flight_year)))\n",
    "print(len(merged_polygons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b223ff",
   "metadata": {},
   "source": [
    "## Processing of extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8714fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\final_extent\"\n",
    "src_tiles = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\final_extent\\tiles\"\n",
    "src_no_landslide = os.path.join(src_tiles, \"no_landslide\")\n",
    "src_images = os.path.join(src_tiles, 'images')\n",
    "src_masks = os.path.join(src_tiles, 'labels')\n",
    "src_masks_bin = os.path.join(src_tiles, 'masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818b758",
   "metadata": {},
   "source": [
    "### Binarization of masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96fc2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_images_to_process = [os.path.join(src_data, x) for x in os.listdir(src_data) if x.split('.')[-1].lower() in ['png','jpg','jpeg'] and 'mask_bin' not in x]\n",
    "list_masks = [x for x in list_images_to_process if 'mask' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d3e9695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [00:24<00:00, 18.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for _, mask in tqdm(enumerate(list_masks),total=len(list_masks)):\n",
    "    img  = Image.open(mask)\n",
    "    img_arr = np.array(img)\n",
    "    img_arr_rgb = img_arr[..., 0:3]\n",
    "    img_bin = np.sum(img_arr_rgb, axis=2)\n",
    "    img_bin[img_bin != 0] = 1\n",
    "    result = Image.fromarray(img_bin.astype(np.uint8))\n",
    "    result_src = ''.join(mask.replace(\"mask\", \"bin\").split('.')[0:-1]) + '.tif'\n",
    "    result.save(result_src)\n",
    "    list_images_to_process.append(result_src)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71468b71",
   "metadata": {},
   "source": [
    "### Splitting images into 512x512 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c790824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_image_overlap(in_path, tile_size=512, overlap=0, save_dir=None, verbose=False):\n",
    "    save_dir = os.path.join(os.path.dirname(in_path), 'tiles') if save_dir == None else save_dir\n",
    "    ext = in_path.split('.')[-1]\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    img = Image.open(in_path)\n",
    "    w, h = img.size\n",
    "\n",
    "    tile_id = 0\n",
    "    for y in range(0, h, tile_size):\n",
    "        for x in range(0, w, tile_size):\n",
    "            # manage if borders reached\n",
    "            if x + tile_size > w:\n",
    "                x = w - tile_size\n",
    "            if y + tile_size > h:\n",
    "                y = h - tile_size\n",
    "\n",
    "            # Crop region (handles border tiles automatically)\n",
    "            tile = img.crop((x, y, x + tile_size, y + tile_size))\n",
    "            output_path = os.path.join(save_dir, ''.join(os.path.basename(in_path).split('.')[:-1]) + f\"_{tile_id}.tif\")\n",
    "            tile.save(output_path)\n",
    "            tile_id += 1\n",
    "    if verbose:\n",
    "        print(f\"Saved {tile_id} overlapping tiles.\")\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "882c1d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1347/1347 [00:52<00:00, 25.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of created tiles:  14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_samples = 0\n",
    "# temp_list = [os.path.join(src_data, x) for x in os.listdir(src_data) if x.endswith('tif')]\n",
    "for _ ,image in tqdm(enumerate(list_images_to_process), total=len(list_images_to_process)):\n",
    "    num_samples += tile_image_overlap(image)\n",
    "\n",
    "print(\"Number of created tiles: \", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7280238a",
   "metadata": {},
   "source": [
    "### Remove tiles that don't contain landslides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba1ae633",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_masks = [os.path.join(src_tiles, x) for x in os.listdir(src_tiles) if 'mask' in x and not 'bin' in x]\n",
    "list_masks_bin = [os.path.join(src_tiles, x) for x in os.listdir(src_tiles) if 'bin' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dacf23ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks:  4685\n",
      "Number of masks bin:  4685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Number of masks: \", len(list_masks))\n",
    "print(\"Number of masks bin: \", len(list_masks_bin))\n",
    "# assert len(list_masks) == len(list_masks_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b7499ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_no_correspondance = []\n",
    "for mask in list_masks:\n",
    "    if not os.path.exists(mask.replace('mask', 'image')):\n",
    "        lst_no_correspondance.append(mask)\n",
    "if len(lst_no_correspondance) > 0:\n",
    "    print(\"The following masks don't have corresponding image:\")\n",
    "    for x in lst_no_correspondance:\n",
    "        print(f\"\\t{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddcfc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4685/4685 [02:33<00:00, 30.53it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(src_no_landslide, exist_ok=True)\n",
    "\n",
    "for _, mask_bin in tqdm(enumerate(list_masks_bin), total=len(list_masks_bin)):\n",
    "    img = Image.open(mask_bin)\n",
    "    img_arr = np.array(img)\n",
    "    assert set(img_arr.flatten()) in [{0}, {1}, {0, 1}]\n",
    "    img.close()\n",
    "    if set(img_arr.flatten()) == {0}:\n",
    "        mask = mask_bin.replace('bin', 'mask')\n",
    "        image = mask.replace('mask', 'image')\n",
    "        os.rename(mask_bin, os.path.join(src_no_landslide, os.path.basename(mask_bin)))\n",
    "        os.rename(mask, os.path.join(src_no_landslide, os.path.basename(mask)))\n",
    "        os.rename(image, os.path.join(src_no_landslide, os.path.basename(image)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c275f",
   "metadata": {},
   "source": [
    "### Sort into subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0aacffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(src_images, exist_ok=True)\n",
    "os.makedirs(src_masks, exist_ok=True)\n",
    "os.makedirs(src_masks_bin, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fefd259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10683/10683 [00:01<00:00, 5751.78it/s]\n"
     ]
    }
   ],
   "source": [
    "list_tiles = [os.path.join(src_tiles, x) for x in os.listdir(src_tiles) if not os.path.isdir(os.path.join(src_tiles,x))]\n",
    "\n",
    "print(len(list_tiles))\n",
    "\n",
    "for _, tile_src in tqdm(enumerate(list_tiles), total=len(list_tiles)):\n",
    "    if 'image' in tile_src:\n",
    "        os.rename(tile_src, os.path.join(src_images, os.path.basename(tile_src).replace('image_', '')))\n",
    "    elif 'mask' in tile_src:\n",
    "        os.rename(tile_src, os.path.join(src_masks, os.path.basename(tile_src).replace('mask_', '')))\n",
    "    elif 'bin' in tile_src:\n",
    "        os.rename(tile_src, os.path.join(src_masks_bin, os.path.basename(tile_src).replace('bin_', '')))\n",
    "    else:\n",
    "        print(f\"No category for: {tile_src}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141bdec",
   "metadata": {},
   "source": [
    "## Find mask with different years\n",
    "### (when comparing '>' and '>=' versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab35c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask_1 = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\masks_1\"\n",
    "src_mask_2 = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\masks_2\"\n",
    "src_img_1 = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\img_1\"\n",
    "src_img_2 = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\img_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0f396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [00:04<00:00, 98.75it/s] \n",
      "100%|██████████| 358/358 [00:03<00:00, 91.55it/s] \n"
     ]
    }
   ],
   "source": [
    "# loading of all masks\n",
    "# lst_masks_1 = [os.path.join(src_mask_1, x) for x in os.listdir(src_mask_1)]\n",
    "# lst_masks_2 = [os.path.join(src_mask_2, x) for x in os.listdir(src_mask_2)]\n",
    "lst_masks_1 = os.listdir(src_mask_1)\n",
    "lst_masks_2 = os.listdir(src_mask_2)\n",
    "\n",
    "lst_masks_1_data = {\n",
    "    x: np.asarray(Image.open(os.path.join(src_mask_1, x))) for _, x in tqdm(enumerate(lst_masks_1), total=len(lst_masks_1))\n",
    "}\n",
    "lst_masks_2_data = {\n",
    "    x: np.asarray(Image.open(os.path.join(src_mask_2, x))) for _, x in tqdm(enumerate(lst_masks_2), total=len(lst_masks_2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7617a661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1400, 4)\n",
      "(1500, 900, 4)\n"
     ]
    }
   ],
   "source": [
    "print(list(lst_masks_1_data.values())[0].shape)\n",
    "print(list(lst_masks_2_data.values())[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db7d7674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [00:06<00:00, 74.06it/s] \n"
     ]
    }
   ],
   "source": [
    "# comparison\n",
    "lst_different_year = []\n",
    "lst_not_found = []\n",
    "lst_found = []\n",
    "lst_problems = []\n",
    "for _, (mask_name, mask_data) in tqdm(enumerate(lst_masks_1_data.items()), total=len(lst_masks_1)):\n",
    "    # is_different = False\n",
    "    # if mask not in lst_masks_2:\n",
    "    #     is_different = True\n",
    "    # elif lst_masks_1_data[mask] != lst_masks_2_data[mask]:\n",
    "    corresponding_masks = [x for x,y in lst_masks_2_data.items() if y.shape == mask_data.shape and (y == mask_data).all()]\n",
    "\n",
    "    if len(corresponding_masks) == 0:\n",
    "        # print(f\"{mask_name} not found\")\n",
    "        lst_not_found.append(mask_name)\n",
    "    elif len(corresponding_masks) > 1:\n",
    "        # print(\"PROBLEM!\")\n",
    "        lst_problems.append(mask_name)\n",
    "    else:\n",
    "        mask_name_2 = corresponding_masks[0]\n",
    "        year_1 = mask_name.split('_')[1]\n",
    "        year_2 = mask_name_2.split('_')[1]\n",
    "        if year_1 != year_2:\n",
    "            lst_different_year.append([\n",
    "                os.path.join(src_mask_1, mask_name),\n",
    "                os.path.join(src_mask_2, mask_name_2),\n",
    "                os.path.join(src_img_1, mask_name.replace('mask', 'image')),\n",
    "                os.path.join(src_img_2, mask_name_2.replace('mask', 'image')),\n",
    "            ])\n",
    "        else:\n",
    "            lst_found.append(mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82a0dcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples the same year:  281\n",
      "Number of samples not the same year:  73\n",
      "Number of samples not found:  95\n",
      "Number of problems:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples the same year: \", len(lst_found))\n",
    "print(\"Number of samples not the same year: \", len(lst_different_year))\n",
    "print(\"Number of samples not found: \", len(lst_not_found))\n",
    "print(\"Number of problems: \", len(lst_problems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b1dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing samples to compare\n",
    "\n",
    "for sample in lst_different_year:\n",
    "    mask = Image.open(sample[0])\n",
    "    img1 = Image.open(sample[2])\n",
    "    img2 = Image.open(sample[3])\n",
    "\n",
    "    fig, axs = plt.subplots(1,3, figsize=(15,4))\n",
    "    axs[0].imshow(img1)\n",
    "    axs[1].imshow(mask)\n",
    "    axs[2].imshow(img2)\n",
    "\n",
    "    year_1 = os.path.basename(sample[2]).split('_')[1]\n",
    "    year_2 = os.path.basename(sample[3]).split('_')[1]\n",
    "    axs[0].set_title(year_1)\n",
    "    axs[2].set_title(year_2)\n",
    "    plt.suptitle(os.path.basename(sample[0]))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2884f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281/281 [00:00<00:00, 445.73it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 554.95it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 419.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2007_25.png already exists!\n",
      "new name: D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2007_117.png\n",
      "---\n",
      "D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2000_1.png already exists!\n",
      "new name: D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2000_2.png\n",
      "---\n",
      "D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2000_2.png already exists!\n",
      "new name: D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2000_3.png\n",
      "---\n",
      "D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2015_43.png already exists!\n",
      "new name: D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2015_105.png\n",
      "---\n",
      "D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2015_95.png already exists!\n",
      "new name: D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2015_106.png\n",
      "---\n",
      "D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2006_7.png already exists!\n",
      "new name: D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2006_23.png\n",
      "---\n",
      "D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2015_5.png already exists!\n",
      "new name: D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2015_107.png\n",
      "---\n",
      "D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2015_26.png already exists!\n",
      "new name: D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\final_extent\\image_2015_108.png\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def copyfile_and_mask(src_img, src_mask, dest):\n",
    "    dest_img = os.path.join(dest, os.path.basename(src_img))\n",
    "    dest_mask = os.path.join(dest, os.path.basename(src_mask))\n",
    "    if os.path.exists(dest_img):\n",
    "        print(f\"{dest_img} already exists!\")\n",
    "        year = os.path.basename(dest_img).split('_')[1]\n",
    "        num_max = np.max([int(x.split('_')[-1].split('.')[0]) for x in os.listdir(os.path.dirname(dest_img)) if year in x])\n",
    "        dest_img = dest_img.split(year+\"_\")[0] + f\"{year}_\" + str(num_max + 1) + \".png\"\n",
    "        dest_mask = os.path.join(os.path.dirname(dest_img), os.path.basename(dest_img).replace('image', 'mask'))\n",
    "        print(f\"new name: {dest_img}\")\n",
    "        print('---')\n",
    "    \n",
    "    shutil.copyfile(src_img, dest_img)\n",
    "    shutil.copyfile(src_mask, dest_mask)\n",
    "\n",
    "# save final selection\n",
    "src_selection = r\"D:\\Terranum_SD\\99_Data\\Landslide\\data\\Bern_glissements_spontane_shpfiles\\test_different_years\\selection_for_different.csv\"\n",
    "list_selection = pd.read_csv(src_selection, sep=';').selection.values\n",
    "final_dest = os.path.join(os.path.dirname(src_mask_1), 'final_extent')\n",
    "os.makedirs(final_dest, exist_ok=True)\n",
    "for _, mask in tqdm(enumerate(lst_found), total=len(lst_found)):\n",
    "    img = mask.replace('mask', \"image\")\n",
    "    copyfile_and_mask(\n",
    "        os.path.join(src_img_1, img),\n",
    "        os.path.join(src_mask_1, mask),\n",
    "        final_dest\n",
    "    )\n",
    "for _, mask in tqdm(enumerate(lst_not_found), total=len(lst_not_found)):\n",
    "    img = mask.replace('mask', \"image\")\n",
    "    copyfile_and_mask(\n",
    "        os.path.join(src_img_1, img),\n",
    "        os.path.join(src_mask_1, mask),\n",
    "        final_dest\n",
    "    )\n",
    "for id_item, val in tqdm(enumerate(list_selection), total=len(list_selection)):\n",
    "# for _, mask in tqdm(enumerate(lst_found), total=len(lst_found)):\n",
    "    source = src_img_1 if val == 1 else src_img_2\n",
    "    img_src = lst_different_year[id_item][2] if val == 1 else lst_different_year[id_item][3]\n",
    "    mask_src = os.path.join(src_mask_1 if val == 1 else src_mask_2, os.path.basename(img_src).replace('image', 'mask'))\n",
    "    # img = mask.replace('mask', \"image\")\n",
    "    copyfile_and_mask(\n",
    "        img_src,\n",
    "        mask_src,\n",
    "        final_dest\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71708ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74d2e41f",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f581f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1124\n"
     ]
    }
   ],
   "source": [
    "list_masks_bin = [os.path.join(src_no_landslide, x) for x in os.listdir(src_no_landslide) if \"mask_bin\" in x and x.endswith('.tif')]\n",
    "print(len(list_masks_bin))\n",
    "for mask in list_masks_bin:\n",
    "    os.rename(mask, mask.replace('mask_bin', 'bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc43527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2248/2248 [00:27<00:00, 82.36it/s] \n"
     ]
    }
   ],
   "source": [
    "list_img = [os.path.join(src_no_landslide, x) for x in os.listdir(src_no_landslide) if not os.path.isdir(os.path.join(src_no_landslide,x)) and \".tif\" not in x]\n",
    "print(len(list_img))\n",
    "for _, img_src in tqdm(enumerate(list_img), total=len(list_img)):\n",
    "    img = Image.open(img_src)\n",
    "    img.save(''.join(img_src.split('.')[:-1]) + '.tif')\n",
    "    os.remove(img_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fb932d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1\n",
      "0  710720621  15972302\n",
      "1   32867394  80349059\n",
      "[[710720621  15972302]\n",
      " [ 32867394  80349059]]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "confmat = pd.read_csv(r\"D:\\GitHubProjects\\Terranum_repo\\LandSlides\\segformerlandslides\\results\\training\\20251216_093805_50_epochs_Bern_b0_from_scratch\\logs\\confmats\\values\\confusion_matrix_ep_6.0.csv\", sep=';', index_col = 0)\n",
    "print(confmat.head())\n",
    "print(confmat.values)\n",
    "print(confmat.values.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
